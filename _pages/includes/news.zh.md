# ğŸ”¥ æœ€æ–°åŠ¨æ€

- *2025.05*: ğŸ§‘â€ğŸ’» å…¥èŒå¾®è½¯äºšæ´²ç ”ç©¶é™¢ï¼ˆMSRAï¼‰å®ä¹   
- *2025.05*: ğŸ‰ 1 ç¯‡è®ºæ–‡è¢« ACL 2025 æ¥æ”¶  
- *2025.05*: ğŸŒŸ 1 ç¯‡è®ºæ–‡è¢« ICML 2025ï¼ˆ**Spotlight**ï¼‰æ¥æ”¶  
- *2024.05*: ğŸ‰ 2 ç¯‡è®ºæ–‡è¢« ACL 2024 æ¥æ”¶  
- *2023.10*: ğŸš€ å¼€å§‹åœ¨ä¼¦æ•¦å›½ç‹å­¦é™¢ï¼ˆKCLï¼‰æ”»è¯»åšå£«  
- *2023.08*: ğŸ‘‹ ç¦»èŒç™¾åº¦ï¼Œæ‹…ä»» NLP ç®—æ³•å·¥ç¨‹å¸ˆï¼ˆ5 ä¸ªæœˆï¼‰  
- *2023.03*: ğŸ“ è·å¾—å“ˆå°”æ»¨å·¥ä¸šå¤§å­¦ï¼ˆæ·±åœ³ï¼‰ç¡•å£«å­¦ä½  
- *2022.03*: ğŸ‰ 1 ç¯‡è®ºæ–‡è¢« SIGIR 2022 æ¥æ”¶  
- *2022.02*: ğŸ‰ 2 ç¯‡è®ºæ–‡è¢« ACL 2022 æ¥æ”¶  

[//]: # ()
[//]: # (- *2023.04*: ğŸ”¥ We release [AudioGPT]&#40;https://github.com/AIGC-Audio/AudioGPT&#41; &#40;â­ï¸6k+&#41;)

[//]: # ()
[//]: # (- *2023.04*: ğŸ‰ One paper &#40;[Make-an-Audio]&#40;https://text-to-audio.github.io/&#41;&#41; is accepted by ICML 2023)

[//]: # ()
[//]: # (- *2023.01*: DiffSinger was introduced in [a very popular video]&#40;https://www.bilibili.com/video/BV1uM411t7ZJ&#41; &#40;2000k+ views&#41; in Bilibili!)

[//]: # ()
[//]: # (- *2023.01*: Three papers are accepted by ICLR 2023!)

[//]: # ()
[//]: # (- *2023.01*: I join [Bytedance AI Lab, Speech & Audio Team]&#40;https://ailab.bytedance.com/&#41; <img src='./images/tiktok.png' style='width: 6em;'> as a research scientist in Singapore!)

[//]: # ()
[//]: # (- *2022.12*: ğŸ‰ My [google scholar]&#40;https://scholar.google.com/citations?user=4FA6C0AAAAAJ&#41; citations have exceeded 2000!)

[//]: # ()
[//]: # (- *2022.02*: I release a modern and responsive academic personal [homepage template]&#40;https://github.com/RayeRen/acad-homepage.github.io&#41;. Welcome to STAR and FORK!)